用户行为日志分析离线数仓项目
项目流程：
    1.用户在客户端上访问和操作等请求，通过Nginx转发到我们的后台server上，后台server记录相应的日志
    2.通过在后台server的服务器上部署Flume agent 进行日志的收集，将日志采集到HDFS上形成原始数据
    3.通过Spark对原始数据进行去脏、字段解析拆分等ETL操作，形成一份清洗好的日志数据写入到HDFS上
    4.通过Hive表关联HDFS上的数据，根据业务利用Spark SQL对我们需要的指标进行统计分析，统计完的结果落入HDFS
    5.利用Sqoop将统计结果导入到关系型数据库中，以供业务团队查询展示
    6.整个项目通过调度框架Azkaban进行定时调度，解决任务之间的依赖关系

组件间的数据衔接关系(HA) 
    1.用户访问请求通过Nginx来做负载均衡
    2.Flume采用汇聚的配置方式，在日志服务器上，通过Flume的Sink Group组+failover的配置方式，汇聚到下游的agent，
    下游agent采用主备的方式将日志输出到HDFS上
    3.通过Spark任务清洗日志，将日志写入到临时目录，再通过删除并重建目标目录，将临时目录下的文件移动到目标目录下，以此保证一旦任务失败可重复执行
    4.通过Azkaban来调度整个任务DAG，即使在某一个任务环节失败了，重新调度任务即可从失败的任务开始继续往下执行，无需重头跑
亮点：
    1.Flume采用汇聚方式，并通过配置实现高可用
    2.数据存储在HDFS上采用了ORC列式存储+ZLIB压缩的方式，减小存储所占磁盘空间，并使ETL过程中尽可能的减少磁盘的I/O
    3.通过采用Spark SQL来加快ETL的速度
    4.Azkaban的任务配置，采用了Azkaban的plugins来配置Spark任务，在需要调整参数的时候直接在Azkaban的Web UI上更改配置
    5.整套数据处理流程都考虑了任务的高可用，任务完成会生成成功标记位，供后续任务判断
    6.Spark SQL统计分析的应用通用化，SQL抽取到外部SQL文件中
   
数据量评估(init==>etl==>sql)、作业/Application数量的评估
数据量评估:
    1.原始日志：一条日志大约 60个字段，大概是300~400个字节左右 一天的日志量大概在2亿~2.5亿条左右
    一天的原始日志量在56G~94G，为减小存储，在Spark ETL之后会对原始数据做压缩处理，采用了bzip2压缩,占用存储空间在16G~27G左右。
    压缩之后即删除原始日志文件，按照公司的规定，压缩文件保留半年的量
    2.经过Spark ETL处理之后，因为对部分字段进行解析、拆分，一条日志60个字段被拆分为80个字段，一行解析后的日志大概在400~500个字节左右，数据量大约为75G~116G左右，
    这里是用Hive把数据文件映射成了Hive表，所以采用了ORC+ZLIB的方式存储，占用空间在12G~19G左右
    3.经过Spark SQL统计分析后的结果数据量较小，这里不做统计
    
作业/Application数量评估：
    1.Spark日志清洗1个，包括了数据的清洗、结果写出到临时目录，删除并重建目标目录，移动数据到目标目录，并做分区关联，最后对原始数据做压缩处理，并删除原始数据文件，
    任务成功会在HDFS上写入成功标记位以供下游做判断
    2.Spark SQL统计分析5个：5个不同的统计任务
    3.Sqoop将统计分析的结果导出到MySQL中的MR作业5个

业务：
    1.热门url top20
    
select t.url,t.cnt,t.total,round(t.cnt*100/t.total,2) as ratio from (select 
url,
count(url) over(partition by url ) cnt,
count(url) over() total 
from ruozedata.ods_access) t 
group by t.url,t.cnt,t.total order by t.cnt desc limit 20
    
    2. 省份的流量和次数
    select province,sum(responsesize) traffics,count(1) cnt from ruozedata.ods_access where d='${dt}' group by province
